{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import struct as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST Dataset\n",
    "## Presplit into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    # Taken from : https://gist.github.com/tylerneylon/ce60e8a06e7506ac45788443f7269e40\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = st.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(st.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_manual():\n",
    "    #  Build data objects from local files \n",
    "    # workflow modified from : https://github.com/sadimanna/idx2numpy_array/blob/master/idx2numpyarray.py\n",
    "\n",
    "    start_time = np.datetime64('now')\n",
    "\n",
    "    training_images_filepath= \"../data/raw/train-images-idx3-ubyte/train-images.idx3-ubyte\"\n",
    "    training_labels_filepath=\"../data/raw/train-labels-idx1-ubyte/train-labels.idx1-ubyte\"\n",
    "\n",
    "    testing_images_filepath= \"../data/raw/t10k-images-idx3-ubyte/t10k-images.idx3-ubyte\"\n",
    "    testing_labels_filepath= \"../data/raw/t10k-labels-idx1-ubyte/t10k-labels.idx1-ubyte\"\n",
    "\n",
    "\n",
    "    x_train=read_idx(training_images_filepath)\n",
    "    x_test=read_idx(testing_images_filepath)\n",
    "    y_train=read_idx(training_labels_filepath)\n",
    "    y_test=read_idx(testing_labels_filepath)\n",
    "\n",
    "    print (\"Y Train Data\")\n",
    "    print (y_train)\n",
    "    print (\"y train shape : \" , y_train.shape)\n",
    "    print (\"x train shape : \" , x_train.shape)\n",
    "\n",
    "    end_time = np.datetime64('now')\n",
    "    print (\"Time of execution : %s seconds\" % str(end_time-start_time))\n",
    "    \n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Train Data\n",
      "[5 0 4 ... 5 6 8]\n",
      "y train shape :  (60000,)\n",
      "x train shape :  (60000, 28, 28)\n",
      "Time of execution : 0 seconds seconds\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test= read_data_manual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load_function(data_load_fx):\n",
    "    if data_load_fx==str(1):\n",
    "        # load data from raw files \n",
    "        print (\"Loading data from raw data\")\n",
    "        x_train, y_train, x_test, y_test= read_data_manual()\n",
    "    else:\n",
    "        # Download the Dataset from AWS s3 bucket online\n",
    "        # the data, split between train and test sets\n",
    "        print (\"Downloading data from AWS\")\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        print (\"Y Train Data\")\n",
    "        print (y_train)\n",
    "        print (\"y train shape : \" , y_train.shape)\n",
    "        print (\"x train shape : \" , x_train.shape)\n",
    "    return (x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Loading Options: \n",
      "To use raw files and convert to array :1 \n",
      "To download from AWS S3 : 2\n",
      "1\n",
      "Loading data from raw data\n",
      "Y Train Data\n",
      "[5 0 4 ... 5 6 8]\n",
      "y train shape :  (60000,)\n",
      "x train shape :  (60000, 28, 28)\n",
      "Time of execution : 0 seconds seconds\n"
     ]
    }
   ],
   "source": [
    "data_load_fx=input(\"\"\" Data Loading Options: \n",
    "To use raw files and convert to array :1 \n",
    "To download from AWS S3 : 2\n",
    "\"\"\")\n",
    "x_train, y_train, x_test, y_test = data_load_function(data_load_fx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peek at the Data format\n",
    "# display(x_train)\n",
    "# display(y_train)\n",
    "\n",
    "# print (\"__\"*10)\n",
    "# display(x_test)\n",
    "# display(y_test)\n",
    "# print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension for train and test data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Convert integer class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Change IMG pixel values to float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#inplace division : returns a value between 0 and 1 for each pixel \n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print Dimensions\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Batch size, # of target classes, and # of epochs\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tensorflow sequential model \n",
    "## to use when there is one input and one output\n",
    "\n",
    "model = Sequential()\n",
    "# add 2d convolutional layer\n",
    "## kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "## activation : Applies the rectified linear unit activation function.\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',input_shape=input_shape))\n",
    "## MaxPooling2D : max poolin operation for 2d spatial data\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# add 2d convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Flatten: Flattens the input - does not affect the batch size\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add Densely connected NN layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#activation of last layer of classification: Interprets result as a probability distribution\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model with losses and metrics \n",
    "##keras.losses.categorical_crossentropy =Computes the categorical crossentropy loss.\n",
    "##keras.optimizers.Adadelta() = Optimizer that implements the Adadelta algorithm.\n",
    "### Adadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension to address two drawbacks: The continual decay of learning rates throughout training.The need for a manually selected global learning rate\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zafrin\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 57s 956us/step - loss: 0.3642 - accuracy: 0.8862 - val_loss: 0.0596 - val_accuracy: 0.9814\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 60s 999us/step - loss: 0.1035 - accuracy: 0.9725 - val_loss: 0.0446 - val_accuracy: 0.9868\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 57s 957us/step - loss: 0.0734 - accuracy: 0.9808 - val_loss: 0.0324 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 57s 957us/step - loss: 0.0611 - accuracy: 0.9839 - val_loss: 0.0341 - val_accuracy: 0.9894\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 57s 952us/step - loss: 0.0521 - accuracy: 0.9870 - val_loss: 0.0282 - val_accuracy: 0.9915\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 56s 935us/step - loss: 0.0450 - accuracy: 0.9884 - val_loss: 0.0276 - val_accuracy: 0.9914\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 58s 961us/step - loss: 0.0385 - accuracy: 0.9903 - val_loss: 0.0320 - val_accuracy: 0.9921\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 57s 951us/step - loss: 0.0355 - accuracy: 0.9909 - val_loss: 0.0265 - val_accuracy: 0.9924\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 55s 915us/step - loss: 0.0315 - accuracy: 0.9915 - val_loss: 0.0246 - val_accuracy: 0.9934\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 57s 958us/step - loss: 0.0287 - accuracy: 0.9923 - val_loss: 0.0263 - val_accuracy: 0.9926\n",
      "The model has successfully trained\n"
     ]
    }
   ],
   "source": [
    "# fit model to data - run batches by epoch\n",
    "hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
    "print(\"The model has successfully trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.026313767190270114\n",
      "Test accuracy: 0.9926000237464905\n"
     ]
    }
   ],
   "source": [
    "# evaluate model by metric\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Zafrin\\\\E2E_ML_Project\\\\notebooks'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model as mnist.h5\n"
     ]
    }
   ],
   "source": [
    "model.save('mnist.h5')\n",
    "print(\"Saving the model as mnist.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
